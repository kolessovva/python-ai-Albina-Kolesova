# Требования к ноутбуку: задание 2 (week2b_read_csv.ipynb)
# Week 2B Notebook Rubric: CSV Analysis Requirements

***

## Назначение файла

Этот файл — критерии оценки Jupyter-ноутбука студента для лабораторной работы 2
курса «Python + AI» (ПетрГУ, 2026).

Смежные файлы:
- task_1_sparql_guide.txt — требования к SPARQL-запросам (задание 1)
- task_3_ideas_visual.txt — идеи и правила для визуализации (задание 3)

***

## Данные и структура

- Один (или больше) CSV-файл из папки data/ репозитория студента (не data/examples/)
- Файл получен SPARQL-запросом студента из лабораторной работы 1
- Ноутбук строится по образцу week2b_read_csv_source.ipynb, но адаптирован
  под свои данные: свои столбцы, свои переменные, свой анализ

***

## Требования к коду

### Шаг 1. Клонирование репозитория

- Клонирование идёт по абсолютному пути /content/{repo},
  чтобы не создавались вложенные директории при повторном запуске

Если шаг 1 выполнен с ошибкой (в первой версии предлагаемого кода
ошибочно создавались вложенные директории), сообщить студенту кратко
и дать две ссылки:
1) https://t.me/PromptMagi — сообщение от 20 февраля описывает проблему
   и способ её исправления
2) https://github.com/componavt/python-ai-template/blob/main/notebooks/week2b_read_csv.ipynb
   — обновлённый файл в репозитории-шаблоне

### Шаг 2A. Чтение CSV

- Файл (или файлы) читается через pd.read_csv(...) без изменений
- Выводится количество загруженных строк и первые несколько строк (display(df.head()))

После чтения: если число строк существенно превышает ожидаемое число           # [NEW]
объектов (например, 527 строк при 88 созвездиях или 3228 строк при             # [NEW]
~500 музеях) — зафиксировать это в Markdown-ячейке и объяснить причину:       # [NEW]
в SPARQL-запросе присутствует многозначное поле (жанры, соседи, архитекторы    # [NEW]
и т.п.), дающее несколько строк на один объект («длинный формат»).             # [NEW]
Это не ошибка данных — это особенность структуры, которую важно                # [NEW]
задокументировать до начала анализа.                                           # [NEW]

### Шаг 2B. Очистка и переименование столбцов

- Столбец с Wikidata URL не удаляется, а переименовывается в "URL" —
  для отладки (можно открыть ссылку и проверить объект в Викиданных)
  (в первой версии кода предлагалось удалять этот столбец;
  разъяснение студентам дано в @PromptMagi в сообщении от 18 февраля)
- Столбцы вида *Label переименовываются в короткие читаемые имена
- Числовые столбцы приводятся к нужному типу через pd.to_numeric(..., errors="coerce")

  Важное исключение для OPTIONAL-полей:
  Поля, полученные через OPTIONAL в SPARQL (например, deathYear, год смерти,
  год снятия с охраны), обрабатываются БЕЗ fillna(0) — NaN здесь несёт
  смысл «данных нет» (объект жив / дата неизвестна).
  Применение fillna(0) к таким полям делает notna() бессмысленным
  и порождает аномальные вычисляемые значения (например, отрицательный
  возраст: 0 − birthYear = −2010).
  Для обязательных числовых полей (birthYear и аналоги) fillna(0) допустим.

- Ячейка идемпотентна: корректно работает при повторном запуске без ошибок.
  Перед операциями drop и rename проверять наличие столбца:
  if 'col' in df.columns: ...
- Идентификация объектов в группировках ведётся по столбцу URL, а не по
  текстовому названию (во избежание слияния омонимов)

Если CSV в «длинном формате»: ячейка 2B документирует это явно и объясняет,  # [NEW]
какое поле создаёт кратность строк. В шаге 4 для такого поля используется     # [NEW]
groupby(url_col).agg(list) или pivot_table — но не до анализа кратности.      # [NEW]
Схлопывать длинный формат в шаге 2B не нужно: сначала изучить его.            # [NEW]

### Шаг 3. Обзор данных

- Выводятся: размер таблицы (shape), список столбцов, первые строки
- Базовая статистика по ключевому числовому столбцу (.describe())
- Распределение значений по категориальному столбцу (.value_counts())

### Шаг 4. Уникальный анализ

- Содержит анализ, специфичный для данных студента (не мультфильмы из образца)

- Код не порождает FutureWarning:
  groupby(..., observed=True) при работе с pd.Categorical (результат pd.cut)

- Перед группировкой или фильтрацией по категориальному столбцу (пол, тип,
  статус и т.п.) выполнять df['col'].value_counts(dropna=False) и проверять
  полный список уникальных значений. Викиданные нередко содержат синонимичные
  или специфичные значения (например, «мужской пол» и «мерин» как варианты
  мужского пола лошади). Такие значения следует явно привести к стандартным
  категориям или задокументировать как отдельную категорию.

- Поиск значений в строковых столбцах: точное совпадение
  (.str.lower().isin(...)) вместо подстрок (.str.contains(...)),
  чтобы не было ложных срабатываний

- Диапазоны bins в pd.cut покрывают весь реальный диапазон данных,
  включая выбросы (отрицательные значения, нули)

- Аномальные записи (выбросы, неожиданные значения) выводятся вместе
  со столбцом URL для ручной проверки в Викиданных.
  Вывод ограничивается первыми 10–20 строками для иллюстрации,
  затем одна итоговая строка с общим числом аномалий.
  Ноутбук — документ для человека: читатель должен осмыслить вывод
  ячейки за 10–15 секунд; тысячи строк подряд нарушают это правило.

- Для каждого поля, полученного через OPTIONAL в SPARQL, подсчитывается
  доля непустых значений: df['field'].notna().mean()
  Выполнять до применения fillna, пока NaN честно обозначает «данных нет».
  Поля с заполненностью менее 10–15% фиксируются отдельно:
  они часто несут интересный аналитический сигнал для задания 3,
  но требуют особого подхода при визуализации.

- Если студент работает с несколькими CSV-файлами, задание включает
  их объединение через pd.merge() по столбцу URL главного объекта
  с проверкой числа совпавших строк

- Если CSV в «длинном формате»: шаг 4 включает явный подсчёт числа          # [NEW]
  уникальных объектов (df['URL'].nunique()) и среднего числа строк           # [NEW]
  на объект, а также анализ самого многозначного поля:                       # [NEW]
  df['field'].value_counts().head(20) — что чаще всего встречается.          # [NEW]
  После этого при необходимости группировка:                                 # [NEW]
  df.groupby('URL')['field'].agg(list).reset_index()                         # [NEW]

Рекомендация (не требование): подсчитать долю объектов с нераспознанным
лейблом — тех, у кого в столбце названия стоит QID вместо читаемого имени
(строка начинается с Q, за которой идут цифры).
Если таких записей менее 5% от общего числа — исправлять CSV не нужно,
достаточно зафиксировать их число. При 5% и выше — стоит разобраться
с причиной (отсутствие русской и английской меток в Викиданных)
и по возможности добавить метки (изменить SPARQL-запрос и запустить его
в сервисе https://query.wikidata.org/).

***

## Требования к тексту (Markdown-ячейки)

- Заголовок и описание цели соответствуют своим данным, а не данным образца
- Текст шага 2B описывает политику сохранения URL (переименование в "URL"),
  а не удаления
- Ячейка Summary отражает именно то, что реально сделано в коде:
  правильное число файлов, правильные столбцы, политика работы со столбцом URL
  соответствует коду (переименование, а не удаление)
- Если CSV в «длинном формате», Markdown-ячейка в шаге 2A или 3 явно          # [NEW]
  фиксирует этот факт с пояснением («527 строк = 88 созвездий × несколько     # [NEW]
  соседей каждое»). Читатель не должен сам догадываться по shape.             # [NEW]
- Все подписи в print() и заголовки графиков написаны на русском и отражают   # [NEW]
  тему студента, а не тему шаблона (мультфильмы, аниме и т.п.).               # [NEW]

***

## Критерии качества

| Критерий          | Требование                                                          |
|---|---|
| Воспроизводимость | Ноутбук запускается с нуля без ошибок (Runtime → Run all)           |
| Повторный запуск  | Клонирование и %cd не создают вложенных папок                       |
| Идемпотентность   | Ячейки корректно работают при повторном запуске без ошибок          |
| Отсутствие Warning| Нет FutureWarning и других предупреждений в выводе                  |
| Корректность      | Нет ложных срабатываний при поиске значений                         |
| Покрытие данных   | Все строки участвуют в анализе, выбросы не теряются                 |
| Отладочность      | URL сохранён — аномалии и QID можно проверить в Викиданных          |
| Заполненность     | Процент непустых значений зафиксирован для каждого OPTIONAL-поля    |
| Согласованность   | Все Markdown-ячейки описывают именно этот датасет и этот код        |
| Объём вывода      | Вывод каждой ячейки читается человеком за 10–15 секунд              |
| Длинный формат    | [NEW] Кратность строк задокументирована, nunique() зафиксирован     |

***
